Great â€” letâ€™s keep the momentum going. Since you said â€œYa please,â€ Iâ€™ll assume you want the next Jekyll page after the landing page. The natural next step is the Installation & Setup page, so Iâ€™ll generate a clean, productionâ€‘ready Markdown file you can drop directly into your Jekyll docs.

If you meant a different page, just tell me and Iâ€™ll pivot smoothly.

---

AI Gateway Ollama â€” Installation & Setup (Jekyll Markdown)

`markdown
---
layout: docs
title: "Installation & Setup"
description: "How to install, configure, and run AI Gateway Ollama locally or in production."
nav_order: 2
---

Installation & Setup

AI Gateway Ollama is designed to be lightweight, modular, and easy to deploy.  
You can run it locally for development or integrate it into a production environment with authentication, rateâ€‘limiting, and audit logging.

---

ğŸ§± Prerequisites

Before installing the gateway, ensure you have:

- Ollama installed and running  
  https://ollama.com/download  
- Git  
- A Unix-like environment (macOS, Linux, WSL2 recommended)
- Optional: Docker (for containerized deployments)

To verify Ollama is running:

`bash
ollama run llama3
`

If the model loads, you're good to go.

---

ğŸ“¦ Clone the Repository

`bash
git clone https://github.com/gitdigital-products/aigateway-ollama
cd aigateway-ollama
`

---

âš™ï¸ Basic Configuration

The gateway reads configuration from:

- config.yml  
- Environment variables  
- Optional .env file  

A minimal example:

`yaml
server:
  host: "0.0.0.0"
  port: 8080

ollama:
  host: "http://localhost:11434"

auth:
  enabled: false
`

For production, youâ€™ll likely enable:

- API keys  
- Rate limits  
- Request logging  
- Model access rules  

These are covered in the Configuration Reference page.

---

â–¶ï¸ Start the Gateway

`bash
./gateway start
`

Or with verbose logging:

`bash
./gateway start --debug
`

Once running, open:

`
http://localhost:8080
`

Youâ€™ll see the API dashboard and model registry.

---

ğŸ³ Running with Docker

If you prefer containerized deployment:

`bash
docker build -t aigateway-ollama .
docker run -p 8080:8080 aigateway-ollama
`

To connect to a remote Ollama node, set:

`bash
docker run -p 8080:8080 \
  -e OLLAMA_HOST=http://your-node:11434 \
  aigateway-ollama
`

---

ğŸ” Production Hardening Checklist

- Enable API key authentication  
- Configure perâ€‘model access rules  
- Turn on structured audit logging  
- Set rate limits and quotas  
- Run behind a reverse proxy (NGINX, Caddy, Traefik)  
- Use HTTPS everywhere  
- Pin model versions for reproducibility  

---

ğŸ§ª Testing the Gateway

Once running, test with:

`bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "llama3",
        "messages": [{"role": "user", "content": "Hello!"}]
      }'
`

You should receive a structured JSON response.

---
